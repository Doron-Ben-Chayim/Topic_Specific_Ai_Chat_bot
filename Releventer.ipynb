{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwkcb4Ik7HCD"
      },
      "source": [
        "##### Install modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivib_AMe7GSd"
      },
      "outputs": [],
      "source": [
        "!pip install praw\n",
        "!pip install -U sentence-transformers\n",
        "!pip install annoy\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2lh9lrk7Cbq"
      },
      "source": [
        "##### Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W-Q5W0k7BYj"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "from requests import Session\n",
        "import urllib3\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from annoy import AnnoyIndex\n",
        "import spacy\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import random\n",
        "from langdetect import detect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zql_a6V7Lp7"
      },
      "source": [
        "##### Useful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZM3EF7X6_ia"
      },
      "outputs": [],
      "source": [
        "def ANN(test_sentence,saved_ann):\n",
        "  model_sent = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "  sentence_embeddings = model_sent.encode(test_sentence)\n",
        "  f = sentence_embeddings.shape[0]\n",
        "  u = AnnoyIndex(f, 'angular')\n",
        "  u.load(f'{saved_ann}') # super fast, will just mmap the file\n",
        "  closest, dist = u.get_nns_by_vector(sentence_embeddings, 1000, include_distances=True) # will find the 1000 nearest neighbors\n",
        "  return closest[:10], dist[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pvZ8ldC7RBS"
      },
      "source": [
        "##### Set up reddit instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZd5OCWe7S7F"
      },
      "outputs": [],
      "source": [
        "### Set up reddit instance\n",
        "reddit = praw.Reddit(\n",
        "    client_id='wAuzk5-Fus41St96ccW7sw',\n",
        "    client_secret=\"FrWDWIXJpptTmeeMkusMj6e9ra4gNg\",\n",
        "    user_agent=\"web_scraper_2000\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51E3cZPn8WbZ"
      },
      "source": [
        "#### Function for Scraper, ANN maker and relevent sentence maker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__K45n_X8V97"
      },
      "outputs": [],
      "source": [
        "def releventor(subreddit_choice):\n",
        "  ### Get list of submission titles\n",
        "  model_sent = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "  for topic in subreddit_choice:\n",
        "      title_list = []\n",
        "      for submission in reddit.subreddit(topic).hot(limit=None):\n",
        "          title_list.append(submission.title)\n",
        "\n",
        "      ### Set up annoy model for scraped titles\n",
        "      sentence_embeddings = model_sent.encode(title_list)\n",
        "      f = len(sentence_embeddings[0])\n",
        "      t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\n",
        "      for i in range(len(sentence_embeddings)):\n",
        "          v = sentence_embeddings[i]\n",
        "          t.add_item(i, v)\n",
        "\n",
        "      t.build(10)  # 10 trees\n",
        "      t.save('titles.ann')\n",
        "\n",
        "      #### Scrape the comments\n",
        "      comment_list = []\n",
        "      for submission in reddit.subreddit(topic).top(limit=100):\n",
        "          try:\n",
        "              submission.comments.replace_more(limit=5)\n",
        "              for comment in submission.comments.list():\n",
        "                  comment_list.append(comment.body)\n",
        "          except:\n",
        "              pass\n",
        "\n",
        "      ### Refine the comments\n",
        "      selected_sent = []\n",
        "      for line in comment_list:\n",
        "        if 'https' in line or 'www' in line:\n",
        "          continue\n",
        "        try:\n",
        "          if detect(line) != 'en':\n",
        "            continue\n",
        "        except:\n",
        "          pass\n",
        "        sent, dist = ANN(line, 'titles.ann')\n",
        "        if mean(dist) < 1 and len(line.strip(\",'\\n \")) > 2:\n",
        "            selected_sent.append(line.strip('\\n'))\n",
        "            if len(selected_sent) == 3000:\n",
        "                break\n",
        "        print(len(comment_list), \"vs\", len(selected_sent))\n",
        "\n",
        "      ## Create refined text document\n",
        "      with open(f'{topic}_relevent.txt', 'w', encoding='utf-8') as f:\n",
        "          for item in selected_sent:\n",
        "              f.write(\"%s\\n\" % item)\n",
        "\n",
        "\n",
        "      #### Create \n",
        "      sentence_embeddings = model_sent.encode(selected_sent)\n",
        "      f = len(sentence_embeddings[0])\n",
        "      t = AnnoyIndex(f, 'angular')\n",
        "      for i in range(len(sentence_embeddings)):\n",
        "          v = sentence_embeddings[i]\n",
        "          t.add_item(i, v)\n",
        "\n",
        "      t.build(10) # 10 trees\n",
        "      t.save(f'{topic}.ann')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XEitHhcJ9-5Y"
      },
      "outputs": [],
      "source": [
        "releventor(['dogs'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Releventer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}